---
title: "Topic Modeling in R"
eval: TRUE
---

## Introduction

An attempt to understand Sherlock Holmes short stories found in Adventures of Sherlock Holmes book by Arthur Conan Doyle.  After inspecting the table of content, the book seems to have 12 stories, one story per chapter.  The analysis is inspired by [Julia Silge](https://juliasilge.com/)’s YouTube video [Topic modeling with R and tidy data principles](https://www.youtube.com/embed/evTuL-RcRpc)

## Download Book

```{r}
library(gutenbergr)  # download books from Project Gutenberg using book ID
library(tidyverse)
library(tidytext)

# Download the book, each line of the book is read into a seperate row
sherlock_raw <- gutenberg_download(48320)
dim(sherlock_raw)
head(sherlock_raw)
tail(sherlock_raw)
```

## Wrangle: Label Stories
```{r}
sherlock <- sherlock_raw %>% 
  
  # determine start of each story/chapter
  mutate(story = ifelse(str_detect(text, "^(A SCANDAL IN BOHEMIA|THE RED-HEADED LEAGUE|A CASE OF IDENTITY|THE BOSCOMBE VALLEY MYSTERY|THE FIVE ORANGE PIPS|THE MAN WITH THE TWISTED LIP|THE ADVENTURE OF THE BLUE CARBUNCLE|THE ADVENTURE OF THE SPECKLED BAND|THE ADVENTURE OF THE ENGINEER’S THUMB|THE ADVENTURE OF THE NOBLE BACHELOR|THE ADVENTURE OF THE BERYL CORONET|THE ADVENTURE OF THE COPPER BEECHES)$"), text, NA)) %>%
  
  # determine lines belonging to each story/chapter by
  # filling down the N/A rows of story column
  fill(story) %>%
  
  # remove the part that does not belong to any story/chapter,
  # i.e, the introduction
  filter(!is.na(story)) %>%
  
  # convert story column to factor
  mutate(story = factor(story))
```


## Wrangle: Put in Tidy Format

The row of `text` column contains multiple words/tokens.  We want to put each word/token of each `text` row into a separate row.  This makes the dataframe follows the tidy format and hence makes it easy to process.

```{r}
tidy_sherlock <- sherlock %>%
  
  # number the rows
  mutate(line = row_number()) %>% 
  
  # break the text column into multiple row where each row contain one token
  unnest_tokens(word, text) %>% 
  
  # remove the stopwords--the rows where the word column is a stopword
  anti_join(stop_words) %>% 
  
  # remove holmes rows which might affect our topic models
  filter(word != "holmes")
```



## Explore tf-idf

-   To see which words are important in each story/chapter, i.e.,the words that appears many times in that story but few or none in the other stories.
-   tf-idf (term frequency-inverse document frequency) is a great exploratory tool before starting with topic modeling

```{r}
library(ggplot2)

tidy_sherlock %>% 
  
  # count number of occurrence of words in stories
  count(story, word, sort = TRUE) %>% 
  
  # compute and add tf, idf, and tf_idf values for words
  bind_tf_idf(word, story, n) %>% 
  
  # group by story
  group_by(story) %>% 
  
  # take top 10 words of each story with highest tf_idf (last column)
  top_n(10) %>% 
  
  # unpack
  ungroup() %>% 
  
  # turn words into factors and order them based on their tf_idf values
  # NOTE: This will not affect order the dataframe rows which is can be
  #   done via the arrange function
  # NOTE: Recording the word column this way is for ggplot to visualize them
  #   as desired from top tf_idf to lowest
  mutate(word = reorder(word, tf_idf)) %>% 
  
  # plot
  ggplot(aes(word, tf_idf, fill = story)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~story, scales = "free", ncol = 3) +
  theme(strip.text.x = element_text(size = 5)) +
  coord_flip()
```


## Implement Topic Modeling

Training the model for the topics

```{r}
library(stm)        # for do topic modeling
library(quanteda)   # great text mining, will be used to structure the input
                    #   to stm

# Convert from tidy form to quanteda form (document x term matrix)
sherlock_stm <- tidy_sherlock %>% 
  count(story, word, sort = TRUE) %>% 
  cast_dfm(story, word, n)

# Train the model
topic_model <- stm(sherlock_stm, K=6, init.type = "Spectral")
```



```{r}
summary(topic_model)
```

## Contribution of Words in Topics

Looking at which words contribute the most in each topic.

```{r}
# Extracting betas and putting them in a tidy format
tm_beta <- tidy(topic_model)

# Visualizing the top words contributing to each topic
tm_beta %>% 
  group_by(topic) %>% 
  # top 10 word in each topic with higest beta (last column)
  top_n(10) %>% 
  ungroup() %>% 
  # turn words into factors and order them based on their tf_idf values
  # NOTE: This will not affect order the dataframe rows which is can be
  #   done via the arrange function
  # NOTE: Recording the word column this way is for ggplot to visualize them
  #   as desired from top tf_idf to lowest
  mutate(term = reorder(term, beta)) %>% 
  ggplot(aes(term, beta, fill = topic)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic, scales = "free", ncol = 3) +
  coord_flip()
```


## Distribution of Topics in Stories

Looking at how the stories are associated with each topic and how strong each association is.

```{r}
# Extracting gammas and putting them in a tidy format
tm_gamma <- tidy(topic_model, matrix = "gamma",
                 # use the names of the stories instead of the default numbers
                 document_names = rownames(sherlock_stm))


# Visualizing the number of stories belonging to each topics and the confidence
#   of the belonging
tm_gamma %>% 
  ggplot(aes(gamma, fill = as.factor(topic))) +
  geom_histogram(show.legend = FALSE) +
  facet_wrap(~topic, ncol = 3)
```



```{r}
# Visualizing how much each topic appear in each story
tm_gamma %>% 
  ggplot(aes(topic, gamma, fill = document)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~document, scales = "free", ncol = 3) +
  theme(strip.text.x = element_text(size = 5))
```

The model did an excellent job strongly associating the stories into one or more topics. This perfect association is rare in the world of topic modeling. The reason behind this perfect association here could be due to the small number of documents that we have.


## References

- Adventures of Sherlock Holmes book by Arthur Conan Doyle on [Project Gutenberg](https://www.gutenberg.org/ebooks/48320)
- [Regular Expressions 101](https://regex101.com/)